{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e244f82-d38b-40ae-8ef3-931ee724d114",
   "metadata": {},
   "source": [
    "## Machine Learning - Final Project ##\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1ebe74-8059-4da6-b019-578fc0f5162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import requests\n",
    "import sqlglot\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from docx import Document\n",
    "from sqlglot import parse_one, ParseError\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any, Tuple, Optional, Iterable\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5695efe4-8b73-4ced-833f-bd9a9f84e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the entire description column (no truncation)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad6d23-c3e3-4336-9177-414551a619c6",
   "metadata": {},
   "source": [
    "---\n",
    "### Load Parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f73dcc-bd90-4d57-8750-7ed9aeffd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "JIRA_EMAIL = os.getenv(\"JIRA_EMAIL\")\n",
    "JIRA_API_TOKEN = os.getenv(\"JIRA_API_TOKEN\")\n",
    "JIRA_BASE_URL = os.getenv(\"JIRA_BASE_URL\")\n",
    "JIRA_PROJECT_KEY = os.getenv(\"JIRA_PROJECT_KEY\")\n",
    "\n",
    "JIRA_FIELD_ASSIGNED_GROUP=os.getenv(\"JIRA_FIELD_ASSIGNED_GROUP\", \"\")\n",
    "\n",
    "JIRA_EPIC_NAME_FIELD = os.getenv(\"JIRA_EPIC_NAME_FIELD\", \"\")\n",
    "JIRA_EPIC_LINK_FIELD = os.getenv(\"JIRA_EPIC_LINK_FIELD\", \"\")\n",
    "TEAM_MANAGED_EPIC_PARENT = os.getenv(\"TEAM_MANAGED_EPIC_PARENT\", \"false\").lower() == \"true\"\n",
    "USE_EPIC_NAME = os.getenv(\"USE_EPIC_NAME\", \"false\").lower() == \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51038e43-1077-41a2-be06-682a44be2afb",
   "metadata": {},
   "source": [
    "---\n",
    "### Read Text from .docx ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb61df6-a643-4d22-b938-2b01dae7971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx_text(filepath: str) -> str:\n",
    "    doc = Document(filepath)\n",
    "    text = []\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if paragraph.text.strip():  # skip empty lines\n",
    "            text.append(paragraph.text.strip())\n",
    "\n",
    "    return \"\\n\".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9fb4e-88c7-4c64-b6f0-430dac0b0731",
   "metadata": {},
   "source": [
    "---\n",
    "### Format ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925b2d57-decc-4edf-9798-34be6af3f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Reusable templates ----\n",
    "format_template = \"\"\"\\\n",
    "Epic: <summary>\n",
    "  Description: <description>\n",
    "  - Task: <summary>\n",
    "    Description: <description>\n",
    "    - Subtask: <summary>\n",
    "      Description: <description>\n",
    "    - Subtask: <summary>\n",
    "      Description: <description>\n",
    "  - Task: <summary>\n",
    "    Description: <description>\n",
    "    ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b2bca-bf10-447f-83e0-8e43b290b726",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 1: Document Structure Agent ###\n",
    "Role: Reads a text and creates tree-like structure in order to be used for the structure of Jira tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e67278-1383-4a5a-b3ff-9f421df6dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_structure_agent(document_text: str, format_template: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in analyzing technical documents and creating structured plans.\n",
    "    Given the following document, extract the high-level structure and identify the Epics, Tasks, and Subtasks.\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. For each item, provide:\n",
    "       - A short **title/summary** (max 10 words, concise, like a Jira title).\n",
    "       - A **Description** (1–3 sentences, expanded from the document's content).\n",
    "    2. Use one Epic as parent for all the Tasks. The Epic Represents the hole document.\n",
    "    3. For Tasks that concerns tabular reports, include all the fields needed in the description and \n",
    "       relate no Subtasks to this Tasks.\n",
    "    4. Return the result in a hierarchical format with an Epic as the parent, followed by Tasks and then Subtasks.\n",
    "    5. Only return the structure in the specified format with no explanation.\n",
    "\n",
    "    Use the following format as guide:\n",
    "    {format_template}\n",
    "\n",
    "    Document:\n",
    "    {document_text}\n",
    "    \"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise, structured planning assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=3000\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3e618a-9a4d-44bc-b93f-c32e214212b7",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 2: Review and Improve Structure ###\n",
    "Role: Reviews the wotk of the previous agent and also processes the feedback from the user, if there is any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3097777-80f3-4c72-b8fd-e131a17ddddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_and_improve_structure(document_structure: str, format_template: str, feedback: str = \"\", model: str = \"gpt-4o-mini\") -> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert at reviewing and improving structured plans.\n",
    "\n",
    "    The structure MUST stay in this hierarchy ONLY:\n",
    "    - Epic\n",
    "      - Task\n",
    "        - Subtask\n",
    "\n",
    "    !! Do not alter the format. Follow the one that is given.\n",
    "\n",
    "    Review the work of a junior assistant and make sure it followed the instructions:\n",
    "    1. For each item, provide:\n",
    "       - A short **title/summary** (max 10 words, concise, like a Jira title).\n",
    "       - A **Description** (1–3 sentences, expanded from the document's content).\n",
    "    2. Use one Epic as parent for all the Tasks. The Epic Represents the hole document.\n",
    "    3. For Tasks that concerns tabular reports, include all the fields needed in the description and \n",
    "       relate no Subtasks to this Tasks.\n",
    "    4. Return the result in a hierarchical format with an Epic as the parent, followed by Tasks and then Subtasks.\n",
    "    5. Only return the structure in the specified format with no explanation.\n",
    "    6. IMPORTANT: Always prioritize and follow feedback provided. \n",
    "       If feedback conflicts with other instructions, \n",
    "       you must follow the feedback first, even if that means ignoring or overriding previous instructions.\n",
    "\n",
    "    If feedback is provided, refine the structure accordingly. The feedback is:\n",
    "    {feedback}\n",
    "\n",
    "    Use the following format as guide:\n",
    "    {format_template}\n",
    "\n",
    "    Review and improve the following structure (output only the improved structure):\n",
    "    {document_structure}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call GPT model via OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a meticulous Jira structure reviewer and improver.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=3000\n",
    "    )\n",
    "\n",
    "    # Extract and return the improved structure\n",
    "    revised_structure = response.choices[0].message.content.strip()\n",
    "    return revised_structure if revised_structure else \"No structured text returned. Please check the prompt or model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e865774-e29e-4f29-99f1-e360e5f28356",
   "metadata": {},
   "source": [
    "---\n",
    "### Staging Area: From Agent's Text to DataFrame ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0e2f74-76e7-4f2f-ba0b-d00fdb0ab9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1) Parse structure (Epic→Task→Subtask) with descriptions\n",
    "# ========================================================\n",
    "\n",
    "def parse_structure_text(structure_text: str) -> List[Dict[str, Any]]:\n",
    "\n",
    "    epics: List[Dict[str, Any]] = []\n",
    "    current_epic = None\n",
    "    current_task = None\n",
    "    current_subtask = None\n",
    "    last_node = None\n",
    "\n",
    "    raw_lines = structure_text.splitlines()\n",
    "    lines = [ln.rstrip() for ln in raw_lines]\n",
    "\n",
    "    def start_node(node_type: str, summary: str) -> Dict[str, Any]:\n",
    "        node = {\"summary\": summary.strip(), \"description\": \"\"}\n",
    "        if node_type == \"epic\":\n",
    "            node[\"tasks\"] = []\n",
    "        elif node_type == \"task\":\n",
    "            node[\"subtasks\"] = []\n",
    "        return node\n",
    "\n",
    "    p_epic    = re.compile(r\"^\\s*(?:[-•]\\s*)?Epic:\\s*(.+)$\", re.IGNORECASE)\n",
    "    p_story   = re.compile(r\"^\\s*(?:[-•]\\s*)?Story:\\s*(.+)$\", re.IGNORECASE)   # treated as Task\n",
    "    p_task    = re.compile(r\"^\\s*(?:[-•]\\s*)?Task:\\s*(.+)$\", re.IGNORECASE)\n",
    "    p_subtask = re.compile(r\"^\\s*(?:[-•]\\s*)?Sub[- ]?task:\\s*(.+)$\", re.IGNORECASE)\n",
    "    p_desc    = re.compile(r\"^\\s*Description:\\s*(.*)$\", re.IGNORECASE)\n",
    "\n",
    "    for ln in lines:\n",
    "        if not ln.strip():\n",
    "            if last_node is not None and last_node[\"description\"]:\n",
    "                last_node[\"description\"] += \"\\n\"\n",
    "            continue\n",
    "\n",
    "        line_no_bullet = re.sub(r\"^\\s*[-•]\\s*\", \"\", ln)\n",
    "\n",
    "        m_epic = p_epic.match(ln)\n",
    "        m_story = p_story.match(ln)\n",
    "        m_task = p_task.match(ln)\n",
    "        m_sub = p_subtask.match(ln)\n",
    "        m_desc = p_desc.match(ln)\n",
    "\n",
    "        if m_epic:\n",
    "            current_epic = start_node(\"epic\", m_epic.group(1))\n",
    "            epics.append(current_epic)\n",
    "            current_task = None\n",
    "            current_subtask = None\n",
    "            last_node = current_epic\n",
    "            continue\n",
    "\n",
    "        # Story lines are treated as Task lines\n",
    "        if m_story or m_task:\n",
    "            if not current_epic:\n",
    "                current_epic = start_node(\"epic\", \"Miscellaneous Epic\")\n",
    "                epics.append(current_epic)\n",
    "            title = (m_story or m_task).group(1)\n",
    "            current_task = start_node(\"task\", title)\n",
    "            current_epic[\"tasks\"].append(current_task)\n",
    "            current_subtask = None\n",
    "            last_node = current_task\n",
    "            continue\n",
    "\n",
    "        if m_sub:\n",
    "            if not current_task:\n",
    "                if not current_epic:\n",
    "                    current_epic = start_node(\"epic\", \"Miscellaneous Epic\")\n",
    "                    epics.append(current_epic)\n",
    "                current_task = start_node(\"task\", \"General Task\")\n",
    "                current_epic[\"tasks\"].append(current_task)\n",
    "            # leaf node for subtask\n",
    "            current_subtask = {\"summary\": m_sub.group(1).strip(), \"description\": \"\"}\n",
    "            current_task[\"subtasks\"].append(current_subtask)\n",
    "            last_node = current_subtask\n",
    "            continue\n",
    "\n",
    "        if m_desc:\n",
    "            if last_node is not None:\n",
    "                last_node[\"description\"] = m_desc.group(1).strip()\n",
    "            continue\n",
    "\n",
    "        # Continuation of last description\n",
    "        if last_node is not None:\n",
    "            if last_node[\"description\"]:\n",
    "                last_node[\"description\"] += (\"\\n\" + line_no_bullet)\n",
    "            else:\n",
    "                last_node[\"description\"] = line_no_bullet\n",
    "\n",
    "    return epics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c775e6-bb7b-47e6-bb4b-3c11223947f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2) Flatten to rows for staging DataFrame\n",
    "# ========================================\n",
    "\n",
    "def flatten_to_staging_rows(parsed: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    epic_i = task_i = sub_i = 0\n",
    "\n",
    "    for epic in parsed:\n",
    "        epic_i += 1\n",
    "        e_id = f\"E{epic_i}\"\n",
    "        rows.append({\n",
    "            \"local_id\": e_id,\n",
    "            \"summary\": epic.get(\"summary\", \"\"),\n",
    "            \"description\": epic.get(\"description\", \"\").strip(),\n",
    "            \"issue_type\": \"Epic\",\n",
    "            \"parent_local_id\": \"\"\n",
    "        })\n",
    "\n",
    "        for task in epic.get(\"tasks\", []):\n",
    "            task_i += 1\n",
    "            t_id = f\"T{task_i}\"\n",
    "            rows.append({\n",
    "                \"local_id\": t_id,\n",
    "                \"summary\": task.get(\"summary\", \"\"),\n",
    "                \"description\": task.get(\"description\", \"\").strip(),\n",
    "                \"issue_type\": \"Task\",\n",
    "                \"parent_local_id\": e_id\n",
    "            })\n",
    "\n",
    "            for sub in task.get(\"subtasks\", []):\n",
    "                sub_i += 1\n",
    "                u_id = f\"U{sub_i}\"\n",
    "                rows.append({\n",
    "                    \"local_id\": u_id,\n",
    "                    \"summary\": sub.get(\"summary\", \"\"),\n",
    "                    \"description\": sub.get(\"description\", \"\").strip(),\n",
    "                    \"issue_type\": \"Subtask\",   # exact spelling per your project\n",
    "                    \"parent_local_id\": t_id\n",
    "                })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3ce829-7a8a-456d-b461-b02a3d52ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3) Built the DataFrame\n",
    "# ======================\n",
    "\n",
    "def build_staging_dataframe(structure_text: str) -> pd.DataFrame:\n",
    "    parsed = parse_structure_text(structure_text)  # your Description-aware parser\n",
    "    rows = flatten_to_staging_rows(parsed)\n",
    "    # Stable, Jira-ready columns\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"local_id\", \"summary\", \"description\", \"issue_type\", \"parent_local_id\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff624cb-f78e-4103-82c4-b387bd37087e",
   "metadata": {},
   "source": [
    "---\n",
    "### User Feedback and Approve ###\n",
    "Role: Prints Outcome and the user gives feedback. If it is not okey, it sends the feedback back to the first agent to start agent with the extra instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55189b10-d425-45af-92a6-4921a3aa6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feedback_and_approve(structure: str) -> Tuple[bool, Optional[pd.DataFrame]]:\n",
    "\n",
    "    while True:\n",
    "        # Always build fresh from the current structure\n",
    "        df = build_staging_dataframe(structure)\n",
    "\n",
    "        print(\"Here is the Jira ticketing structure:\\n\")\n",
    "        display(df)\n",
    "        #show_jira_hierarchy_tree(df, direction=\"UD\")\n",
    "\n",
    "        feedback = input(\"Are you okay with this structure? (yes/no): \").strip().lower()\n",
    "\n",
    "        if feedback == \"yes\":\n",
    "            print(\"\\nThe process continues.\")\n",
    "            return True, df\n",
    "\n",
    "        elif feedback == \"no\":\n",
    "            print(\"Please provide feedback on the changes you would like to make.\")\n",
    "            user_feedback = input(\"Your feedback: \")\n",
    "\n",
    "            # Refine the structure with the feedback\n",
    "            structure = review_and_improve_structure(structure, format_template, feedback=user_feedback)\n",
    "\n",
    "            print(\"\\nThe structure was refined after feedback.\\n\")\n",
    "            #print(structure)\n",
    "\n",
    "            # loop continues — will rebuild df in next iteration\n",
    "\n",
    "        else:\n",
    "            print(\"⚠️ Please answer 'yes' or 'no'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495adad3-fa18-4e98-8143-581da78202a2",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 3: Add Labels ###\n",
    "Role: Adds labels to Jira tickets to help progress/work tracking and also help the project manager and teams understand the responsibilities for each section of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd55ccdf-339e-49da-a846-d17fd395829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = [\n",
    "    \"QA\", \"Reporting\", \"ETL\", \"Data\", \n",
    "    \"Development\", \"Documentation\", \"Business\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ee2fea-ed17-4346-a9b7-f215d6da0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels_to_dataframe(df: pd.DataFrame, model: str = \"gpt-4o-mini\") -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"label\"] = None  # new column\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        context = f\"Summary: {row['summary']}\\nDescription: {row['description']}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert at categorizing work items.\n",
    "\n",
    "        Choose ONE label from this list based on the content:\n",
    "        {\", \".join(labels_list)}\n",
    "\n",
    "        Rules:\n",
    "        - Return only the label (exact spelling, no extra text).\n",
    "        - If multiple could apply, choose the most relevant one.\n",
    "        - Context is from Jira-style summary and description.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise classifier that returns only one label.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        label = resp.choices[0].message.content.strip()\n",
    "        df.at[idx, \"label\"] = label\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6f5d2-df99-424c-ab7d-d9b8bad7238c",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 4: Assigns Ticket to Group ###\n",
    "Role: Assigns Jira ticket to a Group, based on the descirption and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bdfe5bb-323a-4b48-a782-bee8311ba270",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_list = [\n",
    "    \"Business Analysis Team\",\n",
    "    \"Business Intelligence Team\",\n",
    "    \"Configuration Engineer Team\",\n",
    "    \"Data Warehouse Team\",\n",
    "    \"Project Management Office (PMO)\",\n",
    "    \"Quality Assurance Team\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "624ccd30-3d52-42cf-8db9-566e0901b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_groups_to_dataframe(df: pd.DataFrame, model: str = \"gpt-4o-mini\") -> pd.DataFrame:\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"assignee_group\"] = None\n",
    "\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        summary = (row.get(\"summary\") or \"\").strip()\n",
    "        description = (row.get(\"description\") or \"\").strip()\n",
    "        label = (row.get(\"label\") or \"\").strip() if \"label\" in df_copy.columns else \"\"\n",
    "\n",
    "        context = f\"Summary: {summary}\\nDescription: {description}\"\n",
    "        if label:\n",
    "            context += f\"\\nLabel: {label}\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are assigning each work item to exactly ONE delivery group.\n",
    "        \n",
    "        Choose ONE group from this list based on the content:\n",
    "        {\", \".join(groups_list)}\n",
    "        \n",
    "        Guidelines:\n",
    "        - Use the summary, description, and label (if provided).\n",
    "        - Choose the best single fit.\n",
    "        - Return ONLY the group name, with exact casing and spelling. No extra words.\n",
    "        \n",
    "        Item:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=20,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a precise triage assistant. Output exactly one allowed group.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        label = resp.choices[0].message.content.strip()\n",
    "        df.at[idx, \"label\"] = label\n",
    "        \n",
    "        group = resp.choices[0].message.content.strip()\n",
    "        df_copy.at[idx, \"assignee_group\"] = group\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861b048-90c0-466b-96df-83b3d9c933ac",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 5: Find Missing Columns ###\n",
    "Role: Reads the document and the database and returns a list of what it thinks are the missing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e428e0-009d-448d-9b5d-7eed8dcaddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Database Schema\n",
    "def load_schema_txt(path: str | Path) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2c589b-744f-4f3a-8c35-50b820f13595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_fields_df_from_staging(df: pd.DataFrame, schema_text: str, model: str = \"gpt-4o-mini\") -> pd.DataFrame:\n",
    "\n",
    "    # --- validate required columns\n",
    "    required_cols = {\"local_id\", \"summary\", \"description\", \"label\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input df must contain columns: {missing}\")\n",
    "\n",
    "    # --- filter only Reporting\n",
    "    df_filtered = df[df[\"label\"].astype(str).str.strip().str.lower() == \"reporting\"]\n",
    "\n",
    "    rows_out = []\n",
    "\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        local_id    = str(row.get(\"local_id\", \"\")).strip()\n",
    "        summary     = (row.get(\"summary\") or \"\").strip()\n",
    "        description = (row.get(\"description\") or \"\").strip()\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You analyze report specifications against an existing database schema and identify missing fields.\n",
    "        \n",
    "        SCHEMA (human-readable text):\n",
    "        {schema_text.strip()}\n",
    "        \n",
    "        ITEM CONTEXT:\n",
    "        Summary: {summary}\n",
    "        Description: {description}\n",
    "        \n",
    "        TASK:\n",
    "        - If this report clearly requires columns NOT present in the schema, list them.\n",
    "        - For each missing field, return:\n",
    "          • missing_field_name (as mentioned in the report)\n",
    "          • suggested_table (existing table if appropriate, else a sensible new table name)\n",
    "        - If nothing is missing, return an empty array.\n",
    "        \n",
    "        OUTPUT:\n",
    "        Return ONLY valid JSON array, like:\n",
    "        [\n",
    "          {{\"missing_field_name\":\"CustomerSegment\",\"suggested_table\":\"customers\"}}\n",
    "        ]\n",
    "        \"\"\".strip()\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=800,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You compare report requirements to schemas and output only valid JSON arrays.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        text = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "        # parse JSON array; fallback: extract first array\n",
    "        try:\n",
    "            data = json.loads(text)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\[\\s*{.*}\\s*\\]\", text, flags=re.DOTALL)\n",
    "            data = json.loads(m.group(0)) if m else []\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                rows_out.append({\n",
    "                    \"missing_field_name\":   (item.get(\"missing_field_name\") or \"\").strip(),\n",
    "                    \"suggested_table\": (item.get(\"suggested_table\") or \"\").strip(),\n",
    "                    \"parent_local_id\":      local_id\n",
    "                })\n",
    "\n",
    "    # --- build initial DF\n",
    "    out = pd.DataFrame(rows_out, columns=[\n",
    "        \"missing_field_name\", \"suggested_table\", \"parent_local_id\"\n",
    "    ])\n",
    "\n",
    "    # If empty, return as-is\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    # --- normalize missing_field_name to CamelCase (no spaces/underscores); no special Number→No rule\n",
    "    def camelize(name: str) -> str:\n",
    "        tokens = re.findall(r\"[A-Za-z0-9]+\", (name or \"\"))\n",
    "        # Preserve all-caps acronyms (e.g., ID), otherwise Capitalize\n",
    "        parts = [t if t.isupper() and len(t) <= 4 else t.capitalize() for t in tokens]\n",
    "        return \"\".join(parts)\n",
    "\n",
    "    out[\"missing_field_name\"] = out[\"missing_field_name\"].apply(camelize)\n",
    "\n",
    "    # --- deduplicate by normalized missing_field_name, keeping the first occurrence (first report wins)\n",
    "    out = out.drop_duplicates(subset=[\"missing_field_name\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f1835-5e11-47c9-a853-625cad633492",
   "metadata": {},
   "source": [
    "---\n",
    "### Function: Add Subtasks to main DataFrame ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fc2fdd-cf69-4cce-ad10-974961e723ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_add_field_subtasks_interactive(\n",
    "    fields_df: pd.DataFrame,\n",
    "    staging_df: pd.DataFrame,\n",
    "    *,\n",
    "    default_label: str = \"Data\",\n",
    "    default_group: str = \"Data Warehouse Team\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    required_cols = {\"missing_field_name\", \"suggested_table\", \"parent_local_id\"}\n",
    "    missing = required_cols - set(fields_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"fields_df missing columns: {missing}\")\n",
    "\n",
    "    df = staging_df.copy()\n",
    "\n",
    "    # Helper to get next available Subtask local_id\n",
    "    def next_sub_id(existing_ids: pd.Series) -> str:\n",
    "        max_n = 0\n",
    "        for v in existing_ids.dropna().astype(str):\n",
    "            m = re.match(r\"U(\\d+)$\", v.strip(), re.IGNORECASE)\n",
    "            if m:\n",
    "                max_n = max(max_n, int(m.group(1)))\n",
    "        return f\"U{max_n + 1}\"\n",
    "\n",
    "    for _, row in fields_df.iterrows():\n",
    "        field_name = str(row[\"missing_field_name\"]).strip()\n",
    "        table_name = str(row[\"suggested_table\"]).strip()\n",
    "        parent_id = str(row[\"parent_local_id\"]).strip()\n",
    "\n",
    "        # Step 1: Check if field exists in DB\n",
    "        exists_ans = input(f\"Does field '{field_name}' already exist in the database? (yes/no): \").strip().lower()\n",
    "        if exists_ans == \"yes\":\n",
    "            print(f\"Skipping '{field_name}' — already exists.\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Ask if we should add it\n",
    "        add_ans = input(f\"Do you want to add '{field_name}' to table '{table_name}' as a subtask? (yes/no): \").strip().lower()\n",
    "        if add_ans != \"yes\":\n",
    "            print(f\"Skipping '{field_name}' — not adding as subtask.\")\n",
    "            continue\n",
    "\n",
    "        # Step 3: Create the new subtask\n",
    "        new_id = next_sub_id(df[\"local_id\"]) if \"local_id\" in df.columns else \"U1\"\n",
    "        summary = f'Add field \"{field_name}\" to table \"{table_name}\"'\n",
    "        description = f'Implement changes to add the field \"{field_name}\" into table \"{table_name}\", including ETL and database updates.'\n",
    "\n",
    "        new_row = {\n",
    "            \"local_id\": new_id,\n",
    "            \"summary\": summary,\n",
    "            \"description\": description,\n",
    "            \"issue_type\": \"Subtask\",\n",
    "            \"parent_local_id\": parent_id,\n",
    "            \"label\": default_label,\n",
    "            \"assignee_group\": default_group\n",
    "        }\n",
    "\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8bc82-4c35-488f-95f3-6620fdc5fb86",
   "metadata": {},
   "source": [
    "---\n",
    "### Attach Subtask local_id ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "399c02fb-2016-45cf-b95e-1a384acf16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_subtask_local_ids(fields_df: pd.DataFrame, staging_df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    need_f = {\"missing_field_name\", \"suggested_table\", \"parent_local_id\"}\n",
    "    missing = need_f - set(fields_df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"fields_df missing columns: {missing}\")\n",
    "\n",
    "    need_s = {\"local_id\", \"summary\", \"issue_type\", \"parent_local_id\"}\n",
    "    missing_s = need_s - set(staging_df.columns)\n",
    "    if missing_s:\n",
    "        raise ValueError(f\"staging_df missing columns: {missing_s}\")\n",
    "\n",
    "    out = fields_df.copy()\n",
    "    out[\"local_id\"] = \"\"\n",
    "\n",
    "    # Filter only subtasks once for speed\n",
    "    subs = staging_df[staging_df[\"issue_type\"] == \"Subtask\"][[\"local_id\",\"summary\",\"parent_local_id\"]].copy()\n",
    "\n",
    "    for idx, r in out.iterrows():\n",
    "        fname = str(r[\"missing_field_name\"]).strip()\n",
    "        tname = str(r[\"suggested_table\"]).strip()\n",
    "        parent = str(r[\"parent_local_id\"]).strip()\n",
    "        expected_summary = f'Add field \"{fname}\" to table \"{tname}\"'\n",
    "\n",
    "        hit = subs[(subs[\"summary\"] == expected_summary) & (subs[\"parent_local_id\"].astype(str) == parent)]\n",
    "        if not hit.empty:\n",
    "            out.at[idx, \"local_id\"] = str(hit.iloc[0][\"local_id\"])\n",
    "        else:\n",
    "            # Not found — leave local_id empty so you can spot & fix\n",
    "            pass\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7bf23-2a13-4e38-a1ac-5ea118504abe",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 6: Generate SQL Script for Changing the Database Scheme ###\n",
    "Role: Generates SQL script regarding the alteration of database scheme in order to attach it in the Jira ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e11a3de-5ba0-4dc8-a351-38702cceb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alter_sql_per_field_from_staging(\n",
    "    fields_df: pd.DataFrame,\n",
    "    staging_df: pd.DataFrame,\n",
    "    schema_text: str,\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # First, enrich with subtask local_id\n",
    "    merged = attach_subtask_local_ids(fields_df, staging_df)\n",
    "\n",
    "    out_rows = []\n",
    "    for _, r in merged.iterrows():\n",
    "        sub_local_id = (r.get(\"local_id\") or \"\").strip()\n",
    "        if not sub_local_id:\n",
    "            # Skip if we can't find the subtask row\n",
    "            continue\n",
    "\n",
    "        field_name = str(r[\"missing_field_name\"]).strip()\n",
    "        table_name = str(r[\"suggested_table\"]).strip()\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You write concise ANSI SQL DDL.\n",
    "        \n",
    "        Schema (human-readable for context):\n",
    "        {schema_text}\n",
    "        \n",
    "        Task:\n",
    "        Write ONE single-line ALTER TABLE statement to add the column below to the target table.\n",
    "        - If type/length is unclear, choose a sensible default (e.g., VARCHAR(100) or BOOLEAN or DATE).\n",
    "        - Default to NULL unless clearly NOT NULL.\n",
    "        - Do NOT include comments or code fences. End with a semicolon.\n",
    "        \n",
    "        Column to add:\n",
    "        - Table: {table_name}\n",
    "        - Column: {field_name}\n",
    "        \n",
    "        Output:\n",
    "        A single ALTER TABLE ... ADD ...; statement, nothing else.\n",
    "        \"\"\".strip()\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Return only one ANSI SQL ALTER statement; no explanations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        sql = (resp.choices[0].message.content or \"\").strip()\n",
    "        # Clean any accidental code fences/spaces\n",
    "        sql = re.sub(r\"^```[\\s\\S]*?\\n\", \"\", sql)\n",
    "        sql = re.sub(r\"\\n```$\", \"\", sql)\n",
    "        sql = sql.strip()\n",
    "        if not sql.endswith(\";\"):\n",
    "            sql += \";\"\n",
    "\n",
    "        out_rows.append({\n",
    "            \"local_id\": sub_local_id,\n",
    "            \"field_name\": field_name,\n",
    "            \"suggested_table\": table_name,\n",
    "            \"sql_script\": sql\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(out_rows, columns=[\"local_id\", \"field_name\", \"suggested_table\", \"sql_script\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a38abe-0df9-47ec-a45a-ba71dac2edac",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 7: Identify Tabular Reports ###\n",
    "Role: Reads the summary and the description for the reporting Tasks and decides which one concerns Tabular reports in order to pass it to the next Agent and create the SELECT Statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ab3eed-22be-455e-939e-f562cbecbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tabular_reports(staging_df: pd.DataFrame, model: str = \"gpt-4o-mini\") -> pd.DataFrame:\n",
    "\n",
    "    rows = []\n",
    "    for _, r in staging_df.iterrows():\n",
    "        #if str(r.get(\"issue_type\", \"\")).strip().lower() != \"task\":\n",
    "            #rows.append(False)\n",
    "            #continue\n",
    "        if str(r.get(\"label\", \"\")).strip().lower() != \"reporting\":\n",
    "            rows.append(False)\n",
    "            continue\n",
    "\n",
    "        context = f\"Summary: {r.get('summary','')}\\nDescription: {r.get('description','')}\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are an analyst.\n",
    "        Determine if the following describes a TABULAR REPORT (e.g. SELECT query with multiple fields).\n",
    "        Respond ONLY with true or false.\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=5,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Return only 'true' or 'false'.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        text = (resp.choices[0].message.content or \"\").strip().lower()\n",
    "        rows.append(text.startswith(\"t\"))\n",
    "\n",
    "    staging_df[\"is_tabular_report\"] = rows\n",
    "    return staging_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149c053-344a-40c3-90ff-cfa553ec685e",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 8: Generate SELECT Statement for Tabular Reports ###\n",
    "Role: Takes the Tasks from the previous Agent, and based on the description, it generates a SELECT Statement for the Tabular report using the Database Scheme and the missing fields from a previous Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ef035bb-5d61-4a93-af5f-6c884c7e1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_select_sql_for_reports(\n",
    "    staging_df: pd.DataFrame,\n",
    "    schema_text: str,\n",
    "    fields_df: pd.DataFrame | None = None,   # optional\n",
    "    model: str = \"gpt-4o-mini\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    # Build quick lookup: parent_local_id -> [(field, table), ...]\n",
    "    field_map: dict[str, list[tuple[str, str]]] = {}\n",
    "    if fields_df is not None and not fields_df.empty:\n",
    "        for _, f in fields_df.iterrows():\n",
    "            parent = str(f.get(\"parent_local_id\") or \"\").strip()\n",
    "            field_map.setdefault(parent, []).append(\n",
    "                (str(f.get(\"missing_field_name\") or \"\").strip(),\n",
    "                 str(f.get(\"suggested_table\") or \"\").strip())\n",
    "            )\n",
    "\n",
    "    for _, r in staging_df.iterrows():\n",
    "        if not bool(r.get(\"is_tabular_report\")):\n",
    "            continue\n",
    "\n",
    "        local_id = str(r[\"local_id\"])\n",
    "        context = f\"Summary: {r.get('summary','')}\\nDescription: {r.get('description','')}\"\n",
    "\n",
    "        # Attach schema mapping info if available for this task/local_id\n",
    "        extra_fields = field_map.get(local_id, [])\n",
    "        mapping_text = \"\"\n",
    "        if extra_fields:\n",
    "            lines = [f\"- {col} (from {tbl})\" for col, tbl in extra_fields if col and tbl]\n",
    "            if lines:\n",
    "                mapping_text = \"\\nMapped fields from schema:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a SQL expert.\n",
    "        Write a realistic ANSI SQL SELECT statement for the following tabular report.\n",
    "        Use tables and joins consistent with this schema:\n",
    "        {schema_text}\n",
    "        {mapping_text}\n",
    "\n",
    "        Report requirement:\n",
    "        {context}     \n",
    "\n",
    "        Output:\n",
    "        - Return ONLY the SQL query (no explanation, no markdown).\n",
    "        - Include sensible JOINs based on the schema.\n",
    "        - Try using fields from the {schema_text} or {mapping_text} before making calculation using fields from the {schema_text}.\n",
    "        - Use aliases when helpful.\n",
    "        \"\"\".strip()\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=600,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Return only the SQL query; no extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        sql = (resp.choices[0].message.content or \"\").strip()\n",
    "        # Remove code fences if any\n",
    "        sql = re.sub(r\"^```sql\\s*\", \"\", sql, flags=re.IGNORECASE)\n",
    "        sql = re.sub(r\"^```\", \"\", sql)\n",
    "        sql = re.sub(r\"```$\", \"\", sql)\n",
    "        sql = sql.strip()\n",
    "\n",
    "        # Convert literal \\n sequences to actual newlines for readability & better attachments\n",
    "        sql = sql.replace(\"\\\\n\", \"\\n\").strip()\n",
    "\n",
    "        # Optionally normalize multiple blank lines\n",
    "        sql = re.sub(r\"\\n{3,}\", \"\\n\\n\", sql)\n",
    "\n",
    "        out_rows.append({\"local_id\": local_id, \"sql_script\": sql})\n",
    "\n",
    "    if out_rows:\n",
    "        sql_df = pd.DataFrame(out_rows)\n",
    "\n",
    "        # Ensure sql_script column exists\n",
    "        if \"sql_script\" not in staging_df.columns:\n",
    "            staging_df[\"sql_script\"] = None\n",
    "\n",
    "        # Merge and only fill missing values (don’t overwrite existing scripts)\n",
    "        staging_df = staging_df.merge(sql_df, on=\"local_id\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
    "        staging_df[\"sql_script\"] = staging_df[\"sql_script\"].combine_first(staging_df[\"sql_script_new\"])\n",
    "        staging_df = staging_df.drop(columns=[\"sql_script_new\"], errors=\"ignore\")\n",
    "\n",
    "    return staging_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5778c2e-f713-485f-a993-c1f5c3e85ce7",
   "metadata": {},
   "source": [
    "---\n",
    "### Upload Attachments to Jira ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "427c76f6-5510-4d1e-ba99-1300ccd51a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_attachment_to_issue(issue_key: str, file_path: str) -> bool:\n",
    "    url = f\"{JIRA_BASE_URL}/rest/api/3/issue/{issue_key}/attachments\"\n",
    "    headers = {\"Accept\": \"application/json\", \"X-Atlassian-Token\": \"no-check\"}\n",
    "    auth = HTTPBasicAuth(JIRA_EMAIL, JIRA_API_TOKEN)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        files = {\"file\": (os.path.basename(file_path), f, \"application/octet-stream\")}\n",
    "        r = requests.post(url, headers=headers, auth=auth, files=files)\n",
    "    ok = r.status_code in (200, 201)\n",
    "    if ok:\n",
    "        print(f\"\\nAttached {os.path.basename(file_path)} → {issue_key}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(\"Attachment failed:\", r.status_code, r.json())\n",
    "        except Exception:\n",
    "            print(\"Attachment failed:\", r.status_code, r.text)\n",
    "    return ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404b52f-7690-4751-998b-7175bfe8bbed",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 9: Infer Priority per Row ###\n",
    "Role: Creates/updates a priority column using statuses dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5c5d73f-401a-4484-ae37-e6824d4b7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed = [\"Highest\",\n",
    "           \"High\",\n",
    "           \"Medium\",\n",
    "           \"Low\",\n",
    "           \"Lowest\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "979eaa05-02ea-4d99-b47a-af86e2ef10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_priority_predictions(df: pd.DataFrame, model: str = \"gpt-4o-mini\") -> pd.DataFrame:\n",
    "\n",
    "    df2 = df.copy()\n",
    "\n",
    "    by_local = {str(r.local_id): r for _, r in df2.iterrows()}\n",
    "    child_count = {}\n",
    "    for _, r in df2.iterrows():\n",
    "        parent = str(r.get(\"parent_local_id\") or \"\").strip()\n",
    "        if parent:\n",
    "            child_count[parent] = child_count.get(parent, 0) + 1\n",
    "\n",
    "    priorities = []\n",
    "    for _, r in df2.iterrows():\n",
    "        local_id     = str(r.get(\"local_id\") or \"\")\n",
    "        summary      = (r.get(\"summary\") or \"\").strip()\n",
    "        description  = (r.get(\"description\") or \"\").strip()\n",
    "        issue_type   = (r.get(\"issue_type\") or \"\").strip()\n",
    "        label        = (r.get(\"label\") or \"\").strip()\n",
    "        parent_id    = (r.get(\"parent_local_id\") or \"\").strip()\n",
    "\n",
    "        parent_type  = (by_local.get(parent_id).issue_type if parent_id in by_local else \"\") or \"\"\n",
    "        parent_sum   = (by_local.get(parent_id).summary if parent_id in by_local else \"\") or \"\"\n",
    "        has_children = child_count.get(local_id, 0) > 0\n",
    "\n",
    "        context = f\"\"\"IssueType: {issue_type}\n",
    "        Summary: {summary}\n",
    "        Description: {description}\n",
    "        Label: {label}\n",
    "        Parent: {parent_id} ({parent_type}) {parent_sum}\n",
    "        HasChildren: {has_children}\"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a Jira triage assistant. Assign a PRIORITY to this work item.\n",
    "        \n",
    "        Allowed values (choose ONE only):\n",
    "        Highest | High | Medium | Low | Lowest\n",
    "        \n",
    "        Guidelines:\n",
    "        - Highest: blocker, report fails completely, business cannot continue, schema changes needed urgently.\n",
    "        - High: severe issue, report inaccurate or critical ETL, must be resolved quickly.\n",
    "        - Medium: normal functional requirement, important but not blocking other work.\n",
    "        - Low: cosmetic, minor enhancement, documentation, or low impact.\n",
    "        - Lowest: trivial, \"nice to have\", optional extras.\n",
    "        \n",
    "        Important:\n",
    "        - Do NOT default everything to Medium.\n",
    "        - Think carefully about urgency, dependencies, and business impact.\n",
    "        - Output must be exactly one of: Highest, High, Medium, Low, Lowest.\n",
    "        \n",
    "        Item:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=5,\n",
    "            messages=[\n",
    "                {\"role\":\"system\",\"content\":\"Return only one of: Highest | High | Medium | Low | Lowest\"},\n",
    "                {\"role\":\"user\",\"content\":prompt}\n",
    "            ],\n",
    "        )\n",
    "        raw = (resp.choices[0].message.content or \"\").strip()\n",
    "        choice = next((p for p in allowed if p.lower() == raw.lower()), None)\n",
    "        priorities.append(choice)\n",
    "\n",
    "    df2[\"priority\"] = priorities\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ba082-eae3-40e6-ad7e-81e47fd30a97",
   "metadata": {},
   "source": [
    "---\n",
    "### Function: Ask Final Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a32b299-f104-4a82-86eb-b522bf916be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_final_approval(df: pd.DataFrame) -> Tuple[bool, Optional[str]]:\n",
    "\n",
    "    print(\"\\nHere is the final ticketing structure:\")\n",
    "    # Hide super-long sql text by default when printing\n",
    "    if \"sql_script\" in df.columns:\n",
    "        display(df.drop(columns=[\"sql_script\",\"is_tabular_report\"]))\n",
    "    else:\n",
    "        display(df)\n",
    "\n",
    "    while True:\n",
    "        ans = input(\"Are you okay with this final outcome? (yes/no): \").strip().lower()\n",
    "        if ans in (\"yes\", \"no\"):\n",
    "            break\n",
    "        print(\"⚠️ Please answer 'yes' or 'no'.\")\n",
    "\n",
    "    if ans == \"yes\":\n",
    "        return True, None\n",
    "\n",
    "    print(\"Please provide feedback on the changes you would like to make (values only, not structure).\")\n",
    "    fb = input(\"Your feedback: \").strip()\n",
    "    return False, fb or None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c498b6d-5827-4106-b471-eb9bfba37481",
   "metadata": {},
   "source": [
    "---\n",
    "### Agent 10: Apply Feedback to DataFrame\n",
    "Role: Applies feedback to dataframe but it is not changing the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99a74aac-954f-4b25-a332-f86fd278f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feedback_to_dataframe_values(\n",
    "    df: pd.DataFrame,\n",
    "    feedback: str,\n",
    "    *,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    allowed_columns: Optional[List[str]] = None,\n",
    "    extra_context_columns: Optional[List[str]] = None,\n",
    "    temperature: float = 0.0\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    if allowed_columns is None:\n",
    "        # Safe defaults: edit categorization/text but not identifiers or structure\n",
    "        allowed_columns = [\"label\", \"assignee_group\", \"priority\", \"summary\", \"description\"]\n",
    "\n",
    "    if extra_context_columns is None:\n",
    "        extra_context_columns = [\"label\", \"assignee_group\", \"priority\"]\n",
    "\n",
    "    protected = {\"local_id\", \"issue_type\", \"parent_local_id\"}\n",
    "\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # Ensure we never try to edit protected columns\n",
    "    allowed_columns = [c for c in allowed_columns if c not in protected and c in df2.columns]\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are a careful data editor. You will update ONLY the specified columns in a row, \"\n",
    "        \"following the user's feedback. Do not add or remove rows. \"\n",
    "        \"Never change 'local_id', 'issue_type', or 'parent_local_id'. \"\n",
    "        \"Return a JSON object with only the keys that need to be updated for this row. \"\n",
    "        \"If no changes are required, return an empty JSON object {}.\"\n",
    "    )\n",
    "\n",
    "    instructions = f\"\"\"\n",
    "    User feedback (apply globally where appropriate, but decide per-row by context):\n",
    "    {feedback}\n",
    "    \n",
    "    Rules:\n",
    "    - Do NOT modify: local_id, issue_type, parent_local_id.\n",
    "    - You MAY modify only these columns if present: {allowed_columns}.\n",
    "    - Use the row context (summary + description and any extra fields) to decide.\n",
    "    - Output MUST be a SINGLE JSON object with only the fields to change for this row, e.g.:\n",
    "      {{\"label\":\"Data\",\"assignee_group\":\"Data Warehouse Team\"}}\n",
    "    - If no change needed, output: {{}}\n",
    "    \"\"\"\n",
    "\n",
    "    # Process row-by-row\n",
    "    updates: List[Dict[str, Any]] = []\n",
    "    for _, row in df2.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "\n",
    "        # Build compact context for the model\n",
    "        ctx = {\n",
    "            \"local_id\": row_dict.get(\"local_id\"),\n",
    "            \"issue_type\": row_dict.get(\"issue_type\"),\n",
    "            \"parent_local_id\": row_dict.get(\"parent_local_id\"),\n",
    "            \"summary\": row_dict.get(\"summary\", \"\"),\n",
    "            \"description\": row_dict.get(\"description\", \"\"),\n",
    "        }\n",
    "        for c in extra_context_columns:\n",
    "            if c in row_dict:\n",
    "                ctx[c] = row_dict[c]\n",
    "\n",
    "        # Send to model\n",
    "        user_msg = (\n",
    "            \"Row context (use this to understand the row; do not change these directly):\\n\"\n",
    "            + json.dumps(ctx, ensure_ascii=False)\n",
    "            + \"\\n\\n\"\n",
    "            \"Allowed output keys (only if changing): \"\n",
    "            + \", \".join(allowed_columns)\n",
    "            + \"\\n\\n\"\n",
    "            \"Return only a single JSON object (no code fences, no extra text).\"\n",
    "        )\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=150,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": instructions},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        raw = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "        # Be robust to accidental code fences\n",
    "        raw = re.sub(r\"^```json\\s*\", \"\", raw, flags=re.IGNORECASE).strip()\n",
    "        raw = re.sub(r\"^```\", \"\", raw).strip()\n",
    "        raw = re.sub(r\"```$\", \"\", raw).strip()\n",
    "\n",
    "        try:\n",
    "            delta = json.loads(raw)\n",
    "            if not isinstance(delta, dict):\n",
    "                delta = {}\n",
    "        except Exception:\n",
    "            delta = {}\n",
    "\n",
    "        # Apply only allowed columns\n",
    "        clean_delta = {k: v for k, v in delta.items() if k in allowed_columns}\n",
    "        updates.append(clean_delta)\n",
    "\n",
    "    # Merge updates back\n",
    "    for i, upd in enumerate(updates):\n",
    "        if upd:\n",
    "            for k, v in upd.items():\n",
    "                df2.iat[i, df2.columns.get_loc(k)] = v\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6c91c-7fc4-4595-838a-31d08f2a6fd8",
   "metadata": {},
   "source": [
    "---\n",
    "### Create Jira tickets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8384d79-e5d3-468d-b1b4-d6bcfaead4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jira_issue(summary: str,\n",
    "                      description: str | None,\n",
    "                      issue_type: str,                 # \"Epic\" | \"Task\" | \"Subtask\" | etc.\n",
    "                      parent_key: str | None = None,   # Epic for Tasks (team-managed) or Task for Subtasks\n",
    "                      labels: list[str] | None = None, # Jira standard labels (array of strings)\n",
    "                      assigned_group: str | None = None,  # Your “Assigned Group” field (single value)\n",
    "                      custom_fields: dict | None = None,\n",
    "                      priority_name: str | None = None   # <-- added\n",
    "                      ) -> str | None:\n",
    "\n",
    "    # ---- helpers ----\n",
    "    def _auth(): return HTTPBasicAuth(JIRA_EMAIL, JIRA_API_TOKEN)\n",
    "    def _headers(): return {\"Accept\":\"application/json\",\"Content-Type\":\"application/json\"}\n",
    "    def _to_adf(text):\n",
    "        if not text: return None\n",
    "        blocks=[]\n",
    "        for line in str(text).split(\"\\n\"):\n",
    "            if line.strip():\n",
    "                blocks.append({\"type\":\"paragraph\",\"content\":[{\"type\":\"text\",\"text\":line}]})\n",
    "            else:\n",
    "                blocks.append({\"type\":\"paragraph\",\"content\":[]})\n",
    "        while blocks and blocks[-1].get(\"content\")==[]:\n",
    "            blocks.pop()\n",
    "        if not blocks: blocks=[{\"type\":\"paragraph\"}]\n",
    "        return {\"type\":\"doc\",\"version\":1,\"content\":blocks}\n",
    "\n",
    "    def _post(fields):\n",
    "        url=f\"{JIRA_BASE_URL}/rest/api/3/issue\"\n",
    "        return requests.post(url, json={\"fields\":fields}, auth=_auth(), headers=_headers())\n",
    "\n",
    "    # Normalize a Jira label (no spaces)\n",
    "    def _norm_label(s: str) -> str:\n",
    "        s = (s or \"\").strip().lower()\n",
    "        s = re.sub(r\"\\s+\", \"-\", s)\n",
    "        s = re.sub(r\"[^a-z0-9._-]\", \"\", s)\n",
    "        return s\n",
    "\n",
    "    # ---- normalize issuetype ----\n",
    "    t=issue_type.strip().lower()\n",
    "    if t in (\"sub-task\",\"subtask\"): issuetype_name=\"Subtask\"\n",
    "    elif t==\"story\":                 issuetype_name=\"Task\"  # if you’ve removed stories\n",
    "    else:                            issuetype_name=issue_type\n",
    "\n",
    "    # ---- base fields ----\n",
    "    fields={\n",
    "        \"project\":{\"key\":JIRA_PROJECT_KEY},\n",
    "        \"summary\":summary,\n",
    "        \"issuetype\":{\"name\":issuetype_name},\n",
    "    }\n",
    "    adf=_to_adf(description)\n",
    "    if adf: fields[\"description\"]=adf\n",
    "\n",
    "    # Priority (standard Jira field)\n",
    "    if priority_name and priority_name.strip():\n",
    "        fields[\"priority\"] = {\"name\": priority_name.strip()}\n",
    "\n",
    "    # Jira labels (standard field expects list[str])\n",
    "    if labels:\n",
    "        safe_labels = [_norm_label(x) for x in labels if isinstance(x, str) and x.strip()]\n",
    "        if safe_labels:\n",
    "            fields[\"labels\"] = safe_labels\n",
    "\n",
    "    # Optional Epic Name (rarely needed on Cloud Epics)\n",
    "    if issuetype_name.lower()==\"epic\" and USE_EPIC_NAME and JIRA_EPIC_NAME_FIELD:\n",
    "        fields[JIRA_EPIC_NAME_FIELD]=summary\n",
    "\n",
    "    # Custom field example (Suggested Script) on non-Epic\n",
    "    if custom_fields and issuetype_name.lower()!=\"epic\":\n",
    "        if JIRA_FIELD_SUGGESTED_SCRIPT and \"Suggested Script\" in custom_fields:\n",
    "            val=custom_fields[\"Suggested Script\"]\n",
    "            if isinstance(val,str) and val.strip():\n",
    "                fields[JIRA_FIELD_SUGGESTED_SCRIPT]=val.strip()\n",
    "\n",
    "    # Assigned Group (custom field). We’ll try common payload shapes:\n",
    "    # 1) Select list: {\"value\": \"<Group Name>\"}\n",
    "    # 2) Group picker: {\"name\": \"<Group Name>\"}  (fallback on retry if needed)\n",
    "    JIRA_FIELD_ASSIGNED_GROUP = os.getenv(\"JIRA_FIELD_ASSIGNED_GROUP\", \"\")\n",
    "    assigned_group_value = (assigned_group or \"\").strip()\n",
    "    if JIRA_FIELD_ASSIGNED_GROUP and assigned_group_value:\n",
    "        fields[JIRA_FIELD_ASSIGNED_GROUP] = {\"value\": assigned_group_value}\n",
    "\n",
    "    # ---- team-managed style: set parent for ANY non-epic if provided ----\n",
    "    # (Tasks → Epic, Subtasks → Task)\n",
    "    if parent_key and issuetype_name.lower()!=\"epic\":\n",
    "        fields[\"parent\"]={\"key\":parent_key}\n",
    "\n",
    "    # ---- create (attempt 1) ----\n",
    "    r=_post(fields)\n",
    "    if r.status_code==201:\n",
    "        key=r.json().get(\"key\")\n",
    "        print(f\"Created {issuetype_name}: {key}\")\n",
    "        return key\n",
    "\n",
    "    # If server rejects 'parent' for Tasks (company-managed), retry without parent\n",
    "    retry_parent=False\n",
    "    retry_assigned_group=False\n",
    "    try:\n",
    "        body=r.json()\n",
    "        if isinstance(body,dict) and \"errors\" in body:\n",
    "            # parent not allowed on standard issues\n",
    "            if \"parent\" in body[\"errors\"] or any(\"parent\" in (str(v) or \"\") for v in body[\"errors\"].values()):\n",
    "                retry_parent=True\n",
    "            # custom assigned group field rejected -> try alternate shape\n",
    "            if JIRA_FIELD_ASSIGNED_GROUP and JIRA_FIELD_ASSIGNED_GROUP in body[\"errors\"]:\n",
    "                retry_assigned_group=True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Prepare second attempt payload if needed\n",
    "    fields_retry = dict(fields)\n",
    "\n",
    "    # If assigned group failed with {\"value\": ...}, try {\"name\": ...}\n",
    "    if retry_assigned_group and JIRA_FIELD_ASSIGNED_GROUP and assigned_group_value:\n",
    "        ag = fields_retry.get(JIRA_FIELD_ASSIGNED_GROUP)\n",
    "        if isinstance(ag, dict) and \"value\" in ag:\n",
    "            fields_retry[JIRA_FIELD_ASSIGNED_GROUP] = {\"name\": assigned_group_value}\n",
    "\n",
    "    # If parent not allowed for non-subtask, drop it and retry\n",
    "    if retry_parent and parent_key and issuetype_name.lower()!=\"subtask\":\n",
    "        fields_retry.pop(\"parent\", None)\n",
    "\n",
    "    if retry_parent or retry_assigned_group:\n",
    "        r2=_post(fields_retry)\n",
    "        if r2.status_code==201:\n",
    "            key=r2.json().get(\"key\")\n",
    "            #print(f\"Created {issuetype_name} (retry adjustments): {key}\")\n",
    "            return key\n",
    "        else:\n",
    "            try: body2=r2.json()\n",
    "            except Exception: body2=r2.text\n",
    "            print(f\"Failed to create {issuetype_name} on retry. Status:\", r2.status_code)\n",
    "            print(\"Response:\", body2)\n",
    "            return None\n",
    "\n",
    "    # final failure\n",
    "    try: body=r.json()\n",
    "    except Exception: body=r.text\n",
    "    print(f\"Failed to create {issuetype_name}. Status:\", r.status_code)\n",
    "    print(\"Response:\", body)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb3d72e7-727a-4b0e-85a2-1a5e32d024e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jira_from_df(df: pd.DataFrame, *, attach_sql: bool = True, sql_dir: str = \"sql_scripts\") -> dict:\n",
    "\n",
    "    required = {\"local_id\", \"summary\", \"description\", \"issue_type\", \"parent_local_id\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Staging DataFrame missing columns: {missing}\")\n",
    "\n",
    "    with pd.option_context(\"display.max_colwidth\", 200, \"display.width\", 160):\n",
    "        print(\"\\nHere is the Jira ticketing structure, with the extra subtasks:\\n\")\n",
    "        display(df.drop(columns=[\"sql_script\", \"is_tabular_report\"], errors=\"ignore\"))\n",
    "\n",
    "    id_map: dict[str, str] = {}\n",
    "\n",
    "    # -- small local helpers --\n",
    "    def _row_label(row):\n",
    "        if \"label\" in df.columns:\n",
    "            val = (row.get(\"label\") or \"\").strip()\n",
    "            return [val] if val else None\n",
    "        return None\n",
    "\n",
    "    def _row_group(row):\n",
    "        if \"assignee_group\" in df.columns:\n",
    "            val = row.get(\"assignee_group\")\n",
    "            if isinstance(val, str):\n",
    "                return val.strip() or None\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    def _slug(s: str) -> str:\n",
    "        s = (s or \"\").strip().lower()\n",
    "        s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "        return re.sub(r\"_+\", \"_\", s).strip(\"_\") or \"script\"\n",
    "\n",
    "    def _save_sql_file(jira_key: str, summary: str, sql_text: str) -> str:\n",
    "        \"\"\"Save SQL content to <sql_dir>/<jira_key>_<slug(summary)>.sql and return path.\"\"\"\n",
    "        Path(sql_dir).mkdir(parents=True, exist_ok=True)\n",
    "        fname = f\"{jira_key}_{_slug(summary)}.sql\"\n",
    "        path = Path(sql_dir) / fname\n",
    "        sql_clean = sql_text.rstrip()\n",
    "        if not sql_clean.endswith(\";\"):\n",
    "            sql_clean += \";\"\n",
    "        path.write_text(sql_clean + \"\\n\", encoding=\"utf-8\")\n",
    "        return str(path)\n",
    "\n",
    "    def _priority(row):\n",
    "        return (row.get(\"priority\") or \"\").strip() or None\n",
    "\n",
    "    def _create_issue_with_sql(row, issue_type: str, *, parent_key: str | None = None, include_assignee_group: bool = True):\n",
    "        \"\"\"Helper: create issue + attach sql if exists\"\"\"\n",
    "        assignee_group = _row_group(row) if include_assignee_group else None\n",
    "        jira_key = create_jira_issue(\n",
    "            summary=(row.summary or \"\").strip(),\n",
    "            description=(row.description or \"\").strip(),\n",
    "            issue_type=issue_type,\n",
    "            parent_key=parent_key,\n",
    "            labels=_row_label(row),\n",
    "            assigned_group=assignee_group,\n",
    "            priority_name=_priority(row)\n",
    "        )\n",
    "        id_map[str(row.local_id)] = jira_key\n",
    "        #print(f\"\\n{issue_type} created:\", jira_key)\n",
    "\n",
    "        # Attach SQL if requested and available\n",
    "        if attach_sql and jira_key and \"sql_script\" in df.columns:\n",
    "            raw_sql_val = row.get(\"sql_script\", None)\n",
    "            if not pd.isna(raw_sql_val):\n",
    "                sql_text = str(raw_sql_val).strip()\n",
    "                if sql_text and sql_text.lower() != \"nan\":\n",
    "                    try:\n",
    "                        file_path = _save_sql_file(jira_key, (row.summary or \"\"), sql_text)\n",
    "                        upload_attachment_to_issue(jira_key, file_path)\n",
    "                    except Exception as ex:\n",
    "                        print(f\"Warning: failed to attach SQL for {jira_key}: {ex}\")\n",
    "\n",
    "        return jira_key\n",
    "\n",
    "    # 1) Epics\n",
    "    for _, e in df[df.issue_type == \"Epic\"].iterrows():\n",
    "        _create_issue_with_sql(e, \"Epic\")\n",
    "\n",
    "    # 2) Tasks (parent = Epic)\n",
    "    for _, t in df[df.issue_type == \"Task\"].iterrows():\n",
    "        parent_local = str(t.parent_local_id or \"\")\n",
    "        epic_key = id_map.get(parent_local)\n",
    "        _create_issue_with_sql(t, \"Task\", parent_key=epic_key)\n",
    "\n",
    "    # 3) Subtasks (parent = Task)\n",
    "    for _, s in df[df.issue_type == \"Subtask\"].iterrows():\n",
    "        parent_local = str(s.parent_local_id or \"\")\n",
    "        parent_task_key = id_map.get(parent_local)\n",
    "        _create_issue_with_sql(s, \"Subtask\", parent_key=parent_task_key)\n",
    "\n",
    "    return id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b951d6a-8a0d-44be-a59e-dafa63abee84",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Final Pipeline ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77a9db0b-ff78-4bd0-ba19-ba7f6b395164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "    doc_path: str,\n",
    "    schema_text: str | None,\n",
    "    include_sql: bool = True,\n",
    "    include_assignee_group: bool = True\n",
    "):\n",
    "    # ===== 1) STRUCTURE APPROVAL LOOP =====\n",
    "    document_text = read_docx_text(doc_path)\n",
    "    staging_df = None\n",
    "    approved = False\n",
    "\n",
    "    while not approved:\n",
    "        # Agent 1: Parse\n",
    "        structure = document_structure_agent(document_text, format_template)\n",
    "\n",
    "        # Agent 2: Review\n",
    "        improved_structure = review_and_improve_structure(structure, format_template)\n",
    "\n",
    "        # Ask user to approve the improved structure (returns (approved_bool, df OR None))\n",
    "        approved, staging_df = get_user_feedback_and_approve(improved_structure)\n",
    "\n",
    "    # ===== 2) ENRICHMENTS (labels, optional groups, optional SQL, priorities) =====\n",
    "    print(\"\\n🏷️ Adding labels with LLM...\")\n",
    "    staging_df = add_labels_to_dataframe(staging_df)  # adds 'label'\n",
    "\n",
    "    if include_assignee_group:\n",
    "        print(\"\\n👥 Adding assignee group with LLM...\")\n",
    "        staging_df = add_groups_to_dataframe(staging_df)  # adds 'assignee_group'\n",
    "    else:\n",
    "        print(\"\\n[INFO] Skipping assignee group enrichment.\")\n",
    "\n",
    "    # ---------- Optional SQL related steps ----------\n",
    "    if include_sql:\n",
    "        if not schema_text:\n",
    "            print(\"\\n[WARN] include_sql=True but no schema_text provided. Skipping SQL steps.\")\n",
    "        else:\n",
    "            # Find missing fields in reporting tasks\n",
    "            print(\"\\n🔍 Scanning for missing report fields against schema...\")\n",
    "            fields_df = find_missing_fields_df_from_staging(staging_df, schema_text)\n",
    "            if not fields_df.empty:\n",
    "                with pd.option_context(\"display.max_colwidth\", None, \"display.width\", 160):\n",
    "                    print(\"\\nSuggested missing fields:\")\n",
    "                    display(fields_df)\n",
    "\n",
    "                print(\"\\nConfirm which ones to append as standardized 'Add field to table' subtasks...\")\n",
    "                if include_assignee_group:\n",
    "                    staging_df = append_add_field_subtasks_interactive(fields_df, staging_df)\n",
    "                else:\n",
    "                    staging_df = append_add_field_subtasks_interactive(fields_df, staging_df, default_group = \"\")\n",
    "            else:\n",
    "                print(\"No missing fields detected by the agent.\")\n",
    "\n",
    "            # Parse standardized subtasks into (missing_field_name, suggested_table, parent_local_id)\n",
    "            subtasks = staging_df[staging_df[\"issue_type\"] == \"Subtask\"].copy()\n",
    "            parsed_rows = []\n",
    "            pat = re.compile(r'^Add field\\s+\"([^\"]+)\"\\s+to table\\s+\"([^\"]+)\"$', re.IGNORECASE)\n",
    "            for _, r in subtasks.iterrows():\n",
    "                m = pat.match((r.get(\"summary\") or \"\").strip())\n",
    "                if not m:\n",
    "                    continue\n",
    "                parsed_rows.append({\n",
    "                    \"missing_field_name\": m.group(1).strip(),\n",
    "                    \"suggested_table\":    m.group(2).strip(),\n",
    "                    \"parent_local_id\":    str(r.get(\"parent_local_id\") or \"\").strip()\n",
    "                })\n",
    "            fields_for_sql = pd.DataFrame(\n",
    "                parsed_rows,\n",
    "                columns=[\"missing_field_name\", \"suggested_table\", \"parent_local_id\"]\n",
    "            )\n",
    "\n",
    "            # Generate one ALTER per subtask and merge into staging_df.sql_script\n",
    "            if not fields_for_sql.empty:\n",
    "                print(\"\\n🛠️ Generating ALTER TABLE statements (one per subtask)...\")\n",
    "                ddl_df = generate_alter_sql_per_field_from_staging(\n",
    "                    fields_df=fields_for_sql,\n",
    "                    staging_df=staging_df,\n",
    "                    schema_text=schema_text\n",
    "                )\n",
    "                if not ddl_df.empty:\n",
    "                    staging_df = staging_df.merge(\n",
    "                        ddl_df[[\"local_id\", \"sql_script\"]],\n",
    "                        on=\"local_id\",\n",
    "                        how=\"left\"\n",
    "                    )\n",
    "                    if \"sql_script\" in staging_df.columns:\n",
    "                        staging_df[\"sql_script\"] = staging_df[\"sql_script\"].apply(\n",
    "                            lambda x: x.strip() if isinstance(x, str) and x.strip() else None\n",
    "                        )\n",
    "                else:\n",
    "                    print(\"No DDL scripts generated.\")\n",
    "            else:\n",
    "                print(\"No standardized 'Add field ...' subtasks found; skipping DDL generation.\")\n",
    "\n",
    "            # Identify tabular reports, then generate SELECT SQL for them\n",
    "            print(\"\\n📌 Identifying tabular report tasks...\")\n",
    "            staging_df = identify_tabular_reports(staging_df)\n",
    "\n",
    "            print(\"\\n📊 Generating SELECT SQL queries for tabular reports...\")\n",
    "            staging_df = generate_select_sql_for_reports(\n",
    "                staging_df=staging_df,\n",
    "                schema_text=schema_text,\n",
    "                fields_df=fields_for_sql  # enrich queries with mapped fields if available\n",
    "            )\n",
    "\n",
    "    # Priority prediction after all content is in place\n",
    "    print(\"\\n🔺 Predicting priority for each item...\")\n",
    "    staging_df = add_priority_predictions(staging_df)  # adds 'priority'\n",
    "\n",
    "    # ===== 3) FINAL DATAFRAME APPROVAL LOOP (values-only edits) =====\n",
    "    final_ok = False\n",
    "    while not final_ok:\n",
    "        final_ok, fb = ask_final_approval(staging_df)\n",
    "        if final_ok:\n",
    "            break\n",
    "\n",
    "        if fb:\n",
    "            print(\"\\nApplying your feedback. Please wait...\")\n",
    "            staging_df = apply_feedback_to_dataframe_values(\n",
    "                staging_df,\n",
    "                feedback=fb,\n",
    "                allowed_columns=[\"label\", \"assignee_group\", \"priority\", \"summary\", \"description\"],\n",
    "                extra_context_columns=[\"label\", \"assignee_group\", \"priority\"],\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.0\n",
    "            )\n",
    "        else:\n",
    "            print(\"No feedback provided. Please approve or provide value-level feedback.\")\n",
    "\n",
    "    # ===== 4) CREATE JIRA ISSUES =====\n",
    "    print(\"\\n📝 Creating Jira issues...\")\n",
    "    id_map = create_jira_from_df(staging_df, attach_sql=bool(include_sql), sql_dir=\"sql_scripts\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd3de0-afab-47d5-989f-b0b7fadf8f8b",
   "metadata": {},
   "source": [
    "---\n",
    "### Run the pipeline ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef3fb726-c564-49a3-b440-387f663db7ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Document Path\n",
    "doc_path = \"/Users/georgiospefanis/Desktop/Project_Documents/Technical_Document.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9d5ef65-998a-4ae2-843f-63e0b1ecd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Database Schema\n",
    "schema_text = load_schema_txt(\"/Users/georgiospefanis/Desktop/Project_Documents/Database_Schema.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2bb1c0-859d-4d44-b016-2e12a6551e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Full run WITH SQL (requires schema_text and no Assignee Groups)\n",
    "run_pipeline(doc_path, schema_text, include_sql=True, include_assignee_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d8cd8-a1b7-4300-9907-50b405c5cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Fast run WITHOUT SQL (no schema needed)\n",
    "#run_pipeline(doc_path, schema_text=None, include_sql=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
